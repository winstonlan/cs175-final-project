{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ryan/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input\n",
    "from tensorflow.python.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to read csv file into colaboratory:\n",
    "# !pip install -U -q PyDrive\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google import colab\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# # 1. Authenticate and create the PyDrive client.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)\n",
    "\n",
    "# #2. Get the file\n",
    "# downloaded = drive.CreateFile({'id':'1meytppCXQ0WRmngIB5_ORuOWsusRHBch'}) # replace the id with id of file you want to access\n",
    "# downloaded.GetContentFile('handwritten.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will implement [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM] and write script to print accuracy of models on test and train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/handwritten_data_785.csv\", encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data = data.values\n",
    "np.random.shuffle(data)\n",
    "X, y = data[:,1:], data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297629, 784)\n",
      "(74408, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#images are 28x28\n",
    "img_size = 28\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Tuple with height, width and depth used to reshape arrays.\n",
    "# This is used for reshaping in Keras.\n",
    "img_shape_full = (img_size, img_size, 1)\n",
    "\n",
    "num_channels = 1\n",
    "num_classes = 26\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_size, img_size, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_size, img_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_image = np.mean(X_train, axis=0).astype(np.int64)\n",
    "X_train = (X_train - mean_image)/255\n",
    "X_test = (X_test - mean_image)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_one_hot = to_categorical(y_train)\n",
    "test_Y_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: One layer of each ([conv-relu-pool] -> [affine]-> [softmax or SVM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, deeper networks is always better, at the cost of more data and increased complexity of learning.\n",
    "Minibatch size is usually set of few hundreds. \n",
    "You should initially use fewer filters and gradually increase and monitor the error rate to see how it is varying.\n",
    "Very small filter sizes will capture very fine details of the image. On the other hand having a bigger filter size \n",
    "will leave out minute details in the image.\n",
    "https://www.quora.com/How-can-I-decide-the-kernel-size-output-maps-and-layers-of-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.layers import LeakyReLU\n",
    "from tensorflow.python.keras import initializers\n",
    "\n",
    "optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_conv1 (Conv2D)         (None, 28, 28, 16)        800       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               401536    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 26)                3354      \n",
      "=================================================================\n",
      "Total params: 405,690\n",
      "Trainable params: 405,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "'''\n",
    "initializer: he_normal\n",
    "A more recent paper on this topic, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification by He et al., \n",
    "derives an initialization specifically for ReLU neurons, reaching the conclusion that the variance of neurons \n",
    "in the network should be 2.0/n. This gives the initialization w = np.random.randn(n) * sqrt(2.0/n), \n",
    "and is the current recommendation for use in practice in the specific case of neural networks with ReLU neurons.\n",
    "'''\n",
    "#convolutional layer\n",
    "model.add(Conv2D(kernel_size=7, strides=1, filters=16, padding='same',\n",
    "                 activation='linear', name='layer_conv1', input_shape=img_shape_full))\n",
    "#reLU activation\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#Pooling Layer\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "#Fully-Connected Layer with 128 outputs\n",
    "model.add(Dense(128, activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#Fully-Connected Layer with Softmax\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "'''\n",
    "For multiclass classification problems like MNIST, cross entropy is typically used as the loss metric\n",
    "'''\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 297629 samples, validate on 74408 samples\n",
      "Epoch 1/10\n",
      "297629/297629 [==============================] - 192s 646us/step - loss: 0.1367 - acc: 0.9628 - val_loss: 0.0831 - val_acc: 0.9780\n",
      "Epoch 2/10\n",
      "297629/297629 [==============================] - 174s 584us/step - loss: 0.0669 - acc: 0.9818 - val_loss: 0.0680 - val_acc: 0.9818\n",
      "Epoch 3/10\n",
      "297629/297629 [==============================] - 173s 581us/step - loss: 0.0516 - acc: 0.9860 - val_loss: 0.0595 - val_acc: 0.9842\n",
      "Epoch 4/10\n",
      "297629/297629 [==============================] - 175s 589us/step - loss: 0.0410 - acc: 0.9888 - val_loss: 0.0538 - val_acc: 0.9860\n",
      "Epoch 5/10\n",
      "297629/297629 [==============================] - 175s 589us/step - loss: 0.0335 - acc: 0.9907 - val_loss: 0.0522 - val_acc: 0.9873\n",
      "Epoch 6/10\n",
      "297629/297629 [==============================] - 174s 585us/step - loss: 0.0276 - acc: 0.9924 - val_loss: 0.0489 - val_acc: 0.9881\n",
      "Epoch 7/10\n",
      "297629/297629 [==============================] - 174s 585us/step - loss: 0.0237 - acc: 0.9933 - val_loss: 0.0457 - val_acc: 0.9891\n",
      "Epoch 8/10\n",
      "297629/297629 [==============================] - 174s 586us/step - loss: 0.0208 - acc: 0.9942 - val_loss: 0.0461 - val_acc: 0.9891\n",
      "Epoch 9/10\n",
      "297629/297629 [==============================] - 182s 610us/step - loss: 0.0181 - acc: 0.9949 - val_loss: 0.0461 - val_acc: 0.9900\n",
      "Epoch 10/10\n",
      "297629/297629 [==============================] - 172s 577us/step - loss: 0.0166 - acc: 0.9955 - val_loss: 0.0462 - val_acc: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x104f15d68>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,\n",
    "          y=train_Y_one_hot,\n",
    "          epochs=10, batch_size=128,verbose=1, validation_data=(X_test, test_Y_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>without he_normal weights initialization and 0.01 biases initialization:</strong>\n",
    "\n",
    "Train on 297629 samples, validate on 74408 samples\n",
    "\n",
    "Epoch 1/1\n",
    "loss: 0.1502 - acc: 0.9591 \n",
    "\n",
    "val_loss: 0.0927 - val_acc: 0.9762\n",
    "\n",
    "<strong>using he_normal weight initialization and 0.01 biases initialization actually decreased accuracy by 0.0014</strong>\n",
    "\n",
    "Train on 297629 samples, validate on 74408 samples\n",
    "\n",
    "Epoch 1/1\n",
    "\n",
    "loss: 0.1835 - acc: 0.9494 - \n",
    "\n",
    "val_loss: 0.0963 - val_acc: 0.9748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
